---
title: "Decision Trees for Classification"
author: "Jaime Davila"
date: "4/22/2024"
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
library(tidymodels)
tidymodels_prefer(quiet=FALSE)
library(rpart)
library(rpart.plot)
```

Today's class is inspired by the following blog post from [Julia Silge](https://juliasilge.com/blog/scooby-doo/). We will be using a dataset collated from the popular animated series, Scooby Doo:

```{r}
scooby_tbl <- read_csv("~/Mscs 341 S24/Class/Data/scooby.csv", 
                       col_types = "nnfc")

table(scooby_tbl$monster_real)
```

We are interested in determining whether the monster in the episode is real or fake. To do that we will be using the year that the episode was aired and the rating the episode got in imdb. The following plot shows the relationship across variables:

```{r}
scooby_tbl |>
   ggplot(aes(imdb, year_aired))+
    geom_jitter(alpha = 0.7, width = 0.05, 
              height = 0.2, aes(color = monster_real))
```

As usual we will set-up our training/testing dataset:

```{r}
set.seed(123)
scooby_split <- initial_split(scooby_tbl)
scooby_train_tbl <- training(scooby_split)
scooby_test_tbl <- testing(scooby_split)
```

# Recursive partitioning trees

In our previous class we used trees to predict a continuous variable. This time around we will be using a tree to predict a categorical variable (whether the monster is real or fake). Notice than in the following code:

* We use the library `rpart` 
* We will be making use of trees with tree depth of 1 (notice the parameter `tree_depth` below)

```{r}
scooby_model <-
  decision_tree(tree_depth=1) |>
  set_mode("classification") |>
  set_engine("rpart")

scooby_recipe <- recipe(monster_real ~ imdb+year_aired,
                 data=scooby_train_tbl)

scooby_wflow <- workflow() |>
    add_recipe(scooby_recipe) |>
    add_model(scooby_model) 

scooby_fit <- fit(scooby_wflow, scooby_train_tbl)
```


One way is to visualize your model and the testing data when you have only two predictors is to use the library `partree`:

```{r}
library(parttree)
scooby_train_tbl |>
  ggplot(aes(imdb, year_aired)) +
  geom_parttree(data = scooby_fit, 
                aes(fill = monster_real), alpha = 0.2) +
  geom_jitter(alpha = 0.7, width = 0.05, 
              height = 0.2, aes(color = monster_real))
```


We can also get a text output of our tree by using `scooby_fit`

```{r}
scooby_fit
```

Finally we can visualize our decision tree is to use the library `rpart.plot`

```{r}
library(rpart.plot)
scooby_fit |>
  extract_fit_engine() |>
  rpart.plot(roundint=FALSE)
```


1. 
    a. Talk for a couple of minutes to the people in your group about how to interpret all of the elements of the previous visualization. What does the color of each node represents? How about the numbers 0.18 and 93% on the leftmost leaf?

    b. Look in page 347 of PDF of (ISLR) (page 336 of the printed copy) for the definition of Gini index. Calculate it by hand for the region   represented by the left branch of our tree (`imdb>=6.2`). What would be the value of the Gini index if all the observations correspond to `fake` monsters? What would be the value of the Gini index if all of the observations were equally divided across `fake` and `real`?
    
```{r}
0.18*(0.82) + 0.82*(0.18) # Gini index for imdb>=6.2
1.0*0 # Gini index for one dominant class
2*0.5*0.5 # Gini index for equal observations
```

    c. In the same page of ISLR, look for the definition of *entropy* What is the entropy for the left branch of our tree (`imdb>6.2`)? What would the entropy be if 99% of all observations corresponded to `fake` monsters? What would be the value of entropy if all of the observations were equally divided across `fake` and `real`?

```{r}
-((0.18 * log(0.18)) + (0.82 * log(0.82)))
-((0.01 * log(0.01)) + (0.99 * log(0.99)))
-((0.5 * log(0.5)) + (0.5 * log(0.5)))
```

2. How does the classification tree algorithm work to create the tree with just one level? How does the algorithm work if you decide to create a tree with two levels?

3. Calculate the accuracy and the confusion matrix using your testing dataset

```{r}
augment(scooby_fit, new_data=scooby_test_tbl ) |>
   accuracy(truth = monster_real, estimate = .pred_class)

augment(scooby_fit, new_data=scooby_test_tbl ) |>
  conf_mat(truth = monster_real, estimate = .pred_class)
```




# Optimizing our parameters

In this example we are interested in finding the optimal `cp` and `tree_depth` using a cross-validation approach. We will follow these steps:

* Make sure the parameters that you will be optimizing are tuneable.
* Create a cross validation dataset.
* Create a grid for searching the parameters in your dataset
* Use `tune_grid` to optimize your function across the values of your grid

```{r}
# Create the model with tuneable parameters
scooby_model <-
  decision_tree(tree_depth=tune(),
                cost_complexity=tune()) |>
  set_mode("classification") |>
  set_engine("rpart")

scooby_recipe <- recipe(monster_real ~ imdb+year_aired,
                 data=scooby_train_tbl)

scooby_wflow <- workflow() |>
    add_recipe(scooby_recipe) |>
    add_model(scooby_model) 
  
# Create the cross-validation dataset
set.seed(1234)
scooby_folds <- vfold_cv(scooby_train_tbl, v = 10)
```

For creating the grid we will leverage the functions `cost_complexity()` and `tree_depth()`:

```{r}
#Set up the grid
(scooby_grid <- 
  grid_regular(cost_complexity(), tree_depth(), levels = 4))
```

Finally we can tune our parameters as follows:

```{r}
scooby_res <-
  tune_grid(
    scooby_wflow,
    resamples = scooby_folds,
    grid = scooby_grid,
    metrics = metric_set(accuracy))

autoplot(scooby_res)
```

4. In your groups briefly discuss the results from the previous visualization. What set of parameters give you the best results on your cross-validation?

Finally we can choose our optimal parameters using `select_by_one_std_err`, finalize our workflow and calculate our accuracy as follows:

```{r}
show_best(scooby_res, metric = "accuracy")
(best_penalty <- select_by_one_std_err(scooby_res, metric = "accuracy", 
                                       -cost_complexity))
scooby_final_wf <- finalize_workflow(scooby_wflow,
                                     best_penalty)
scooby_final_fit <- fit(scooby_final_wf,  scooby_train_tbl)

augment(scooby_final_fit, scooby_test_tbl) |>
  metrics(monster_real, .pred_class)
```

Let's visualize our model  using `parttree` and `rpart.plot`:

```{r}
scooby_train_tbl |>
  ggplot(aes(imdb, year_aired)) +
  geom_parttree(data = scooby_final_fit, aes(fill = monster_real), alpha = 0.2) +
  geom_jitter(alpha = 0.7, width = 0.05, height = 0.2, aes(color = monster_real))

scooby_final_fit |>
  extract_fit_engine() |>
  rpart.plot(roundint=FALSE)
```

# Back to MNIST.

We would like to revisit one of our favorite problems, digit classification, this time using decision trees.

To do that first, let's create a subset of the MNIST dataset with just the digits `1` and `2`, and split into training and testing datasets:

```{r}
set.seed(1234)
digits = c(1,2)
mnist_12 <- read_csv("~/Mscs 341 S24/Class/Data/mnist.csv.gz") |>
  filter(digit %in% digits)|>
  mutate(digit = as.factor(digit))|>
  slice_sample(n=1000)

mnist_split <- initial_split(mnist_12)
train_12_tbl <- training(mnist_split)
test_12_tbl <- testing(mnist_split)
```

And let's keep some plotting functions in case we need them

```{r}
plotImage <- function(dat,size=28){
  imag <- matrix(dat,nrow=size)[,28:1]
  image(imag,col=grey.colors(256), xlab = "", ylab="") 
}

plot_row <- function(tbl) {
  ntbl <- tbl |>
    select(-digit)
  plotImage(as.matrix(ntbl))
}

plot_row(train_12_tbl[1,])
plot_row(train_12_tbl[5,])
plot_row(train_12_tbl[10,])
```

5. Create a decision tree that would distinguish between 1s and 2s with a tree depth of 2 (no need to optimize to set or optimize the `cost_complexity` parameter). Visualize the decision tree using `rpart.plot`. How many pixels does your model use to make a decision? What is the accuracy and the confusion matrix on the testing dataset?

```{r}
digit_model <-
  decision_tree(tree_depth=2) |>
  set_mode("classification") |>
  set_engine("rpart")

digit_recipe <- recipe(digit ~ ., data=train_12_tbl)

digit_wflow <- workflow() |>
    add_recipe(digit_recipe) |>
    add_model(digit_model) 

digit_fit  <- fit(digit_wflow, train_12_tbl)

augment(digit_fit, test_12_tbl) |>
  accuracy(truth=digit, estimate=.pred_class)

augment(digit_fit, test_12_tbl) |>
  conf_mat(truth=digit, estimate=.pred_class)

digit_fit |>
  extract_fit_engine() |>
  rpart.plot(roundint=FALSE)
```


For fun, let's visualize what those pixels are in the original image.

```{r}
mat <- rep(0, 28*28)
mat[552] <- 255
mat[351] <- 125
image(matrix(mat, 28, 28))
```

In decision trees we can quantify the importance of variables in the following way. At each node a single variable is used to partition the data into two homogeneous groups and in doing so maximizes some measure of improvement. The importance of a variable $x$ is the sum of the improvements over all internal nodes of the tree for which $x$ was chosen as the partitioning variable.

R we can use the `vip` library to calculate the variable importance. Notice that in the cases of regression tree we will get the importance of the variables for the optimal value of cp. Finally let's note that we can get the information as a tibble using the function `vip:vi()`

```{r echo=TRUE}
library(vip)

digit_fit |>
  extract_fit_engine() |>
  vip::vip()

imp_tbl <- digit_fit |>
  extract_fit_engine() |>
  vip::vi()
imp_tbl
```

Finally we can create an image that will allow us to visualize the importance of those pixels (features)

```{r}
imp_tbl <- imp_tbl |>
  mutate(col=as.double(str_remove(Variable,"V")))

mat <- rep(0, 28*28)
mat[imp_tbl$col] <- imp_tbl$Importance
image(matrix(mat, 28, 28))
```
