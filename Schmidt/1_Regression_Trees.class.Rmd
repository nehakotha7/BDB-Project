---
title: "Introduction to Regression Trees: Prediction"
author: "Jaime Davila"
date: "4/17/2024"
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup,warning=FALSE, message=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
library(tidymodels)
library(dslabs)
tidymodels_prefer()
library(rpart)
library(rpart.plot)
```

# Introduction 

Today we will be using data from the presidential polls for the 2008 election (Obama vs McCain). Let's start by loading the dataset

```{r}
library(dslabs)
data("polls_2008")
polls_2008_tbl <- tibble(polls_2008)
polls_2008_tbl
```

Notice that we only have two variables, the first one is `day` which measures the day until election day (day 0 is election night) and `margin` which is the average difference margin between Obama and McCain for that day. We can plot our data by doing

```{r}
ggplot(polls_2008_tbl, aes(day, margin))+
  geom_point()
```

# Using regression trees

We are interested in finding the **trend** of the margin using the day as our input variable. In particular we will be assuming that the trend for a period of days will be constant, so using a regression tree seems like the natural choice. So without further do, let's implement our usual steps using `tidymodels()`

* We define our testing/training dataset:

```{r}
set.seed(123)
poll_split <- initial_split(polls_2008_tbl)
poll_train_tbl <- training(poll_split)
poll_test_tbl <- testing(poll_split)
```

* We define our regression tree model. Initially we will settle for a `tree_depth` parameter of 2 and since the margin is a continuous variable we will be using the `"regression"` mode.

```{r}
poll_model <-
  decision_tree(tree_depth=2) |>
  set_mode("regression") |>
  set_engine("rpart")

poll_recipe <- recipe(margin ~ day, data=poll_train_tbl)

poll_wflow <- workflow() |>
    add_recipe(poll_recipe) |>
    add_model(poll_model) 
```

* We train our model using our training data

```{r}
poll_fit  <- fit(poll_wflow, poll_train_tbl)
```

* And we evaluate our model performance using our testing data

```{r}
poll_final_tbl <- augment(poll_fit, poll_test_tbl) 
rmse(poll_final_tbl, margin, .pred)
```

We can visualize our regression tree as a tree

```{r}
poll_fit |>
  extract_fit_engine() |>
  rpart.plot(roundint=FALSE)
```

Or better yet we can see the trend obtained by the regression tree on our original dataset

```{r}
augment(poll_fit, polls_2008_tbl) |>
  ggplot()+
  geom_point(aes(day,margin))+
  geom_step(aes(day,.pred), col="red")
```

# Understanding cost complexity in regression trees

In the following exercises we will be exploring the process of the construction of the regression tree and how to optimize the selection of the parameters for our tree model.

1. Fill out the blanks of the function `calc_rmse_tree` that receives two parameters, `tree_depth` and `cost_complexity`. This function

 * Creates a regression tree with parameters `tree_depth` and `cost_complexity` 
 * Visualizes your tree using `rpart.plot`
 * Calculates the *rmse* on the training data (yes, for the time being we will be calculating the *rmse* on our *training* dataset). 
 
Test your function using `tree_depth`=1,2, while keeping `cost_complexity`=0.1. What happens to your RMSE as you increase the tree depth?

```{r}
calc_rmse <- function(tree_depth, cost_complexity) {
   # Train your model
  
   # Visualize your model
    poll_fit |>
      extract_fit_engine() |>
      rpart.plot(roundint=FALSE) 
  # Calculate and output the rmse

}
```


```{r}
calc_rmse <- function(tree_depth, cost_complexity) {
  # Train your model
  poll_model <-
    decision_tree(tree_depth=tree_depth, cost_complexity=cost_complexity) |>
    set_mode("regression") |>
    set_engine("rpart")

  poll_recipe <- recipe(margin ~ day, data=poll_train_tbl)

  poll_wflow <- workflow() |>
    add_recipe(poll_recipe) |>
    add_model(poll_model) 
  
  poll_fit  <- fit(poll_wflow, poll_train_tbl)

  # Visualize your model
  poll_fit |>
    extract_fit_engine() |>
    rpart.plot(roundint=FALSE) 

  # Calculate and output the rmse
  rmse <- augment(poll_fit, poll_train_tbl) |>
    rmse(margin, .pred) |> 
    pull(.estimate)
  
  rmse
}
```

```{r}
calc_rmse(1,0.1)
calc_rmse(2,0.1)
```

2. In principle, every time that we add a level to our tree we can decrease our RSS (which is  $RMSE^2 \times n$). If we continue this process, we will end up with a tree where every leaf corresponds to a single training observation, which is clearly overfitting.

The `complexity_parameter` (`cp`) controls the number of recursive splits your model takes. Roughly, it does this by measuring the difference in fit (measured by the RSS) when you add a new level in the tree. If the difference in fit is less than the `cp` it stops. 

a. Explain why `calc_rmse(3,0.1)` produces the same results as `calc_rmse(2,0.1)`.

b. Fixing `tree_depth=3`, experiment changing the `cp` parameter so that you get a regression tree of depth 3 (remember we don't count the root in our depth). Change your parameters (`tree_depth` and `cp`) so that you get a regression tree of depth 5.

```{r}
n <- dim(poll_train_tbl)[1]
calc_rmse(2,0.1)^2*n  #Remember RSS=(RMSE^2)*n
calc_rmse(3,0.1)^2*n

calc_rmse(3,0.01)^2*n
calc_rmse(5,0.001)^2*n
```

# Optimizing the cost complexity.

The cost complexity is perhaps **the most important parameter** in a regression tree. It plays a similar role as the penalty in regularized linear models. Trees with optimal cost complexity are called **optimal trees**

In the following exercises we will be exploring how to obtain the optimal cost complexity parameter

3. Discuss briefly in your groups, what strategy would you use to obtain the optimal cost complexity parameter?


We will start by creating a 10-fold cross-validation dataset as well as a grid for potential values of cost complexity. Notice how we leverage the functions `grid_regular()` and `cost_complexity()`


```{r}
# Create the cross-validation dataset
set.seed(31416)
poll_folds <- vfold_cv(poll_train_tbl, v = 10)
poll_grid <- 
  grid_regular(cost_complexity(), levels = 10)
```

4. a. Describe what type of grid we obtained in `poll_grid`.

b. Describe in your own terms how the "one-standard-error" would work in this context. When comparing two regression trees of different depth, which one will you consider **simpler**?

c. Using the following 10-fold cross-validation dataset and grid, find the optimal `cp` using the "one-standard-error" rule. When using the "one-standard-error" rule, we will consider a model **simpler** when `cp` is higher (higher values of cp results in trees with less number of levels).

Calculate the rmse and plot the final model using your testing dataset

```{r}
poll_tune_model <-
  decision_tree(cost_complexity=tune()) |>
  set_mode("regression") |>
  set_engine("rpart")

poll_wflow <- workflow() |>
    add_recipe(poll_recipe) |>
    add_model(poll_tune_model) 

poll_res <-
  tune_grid(
    poll_wflow,
    resamples = poll_folds,
    grid = poll_grid)

autoplot(poll_res, metric="rmse")
show_best(poll_res, metric = "rmse")
```


```{r}
(best_penalty <- select_by_one_std_err(poll_res, 
                                       metric = "rmse", 
                                       -cost_complexity))

poll_final_wf <- finalize_workflow(poll_wflow, best_penalty)
poll_final_fit <- fit(poll_final_wf,  poll_train_tbl)
poll_final_rs <- last_fit(poll_final_wf, poll_split)
collect_metrics(poll_final_rs)
```

```{r}
augment(poll_final_fit, poll_train_tbl) |>
    ggplot()+
    geom_point(aes(day,margin))+
    geom_step(aes(day,.pred), col="red")

poll_fit |>
  extract_fit_engine() |>
  rpart.plot(roundint=FALSE)
```

# No homework due next week!
# Remember your first project report is due on Tu 11/26 at 6pm.



